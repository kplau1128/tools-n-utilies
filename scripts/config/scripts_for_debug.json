{
    "scripts": [
        {
            "name": "GPT-J-6B Measure",
            "env": {
                "QUANT_CONFIG": "../../../optimum-habana/examples/text-generation/quantization_config/maxabs_measure.json"
            },
            "path": "../../../optimum-habana/examples/text-generation/run_generation.py",
            "default_arguments": [
                "--model_name_or_path", "EleutherAI/gpt-j-6b",
                "--use_hpu_graphs",
                "--use_kv_cache",
                "--reuse_cache",
                "--limit_hpu_graphs",
                "--bf16"
            ],
            "extra_arguments": [
                {
                    "max_input_tokens": "128",
                    "max_new_tokens": "128",
                    "batch_size": "1"
                }
            ]
        },
        {
            "name": "GPT-J-6B Quantization",
            "env": {
                "QUANT_CONFIG": "../../../optimum-habana/examples/text-generation/quantization_config/maxabs_quant.json"
            },
            "path": "../../../optimum-habana/examples/text-generation/run_generation.py",
            "default_arguments": [
                "--model_name_or_path", "EleutherAI/gpt-j-6b",
                "--use_hpu_graphs",
                "--use_kv_cache",
                "--reuse_cache",
                "--limit_hpu_graphs",
                "--bf16",
                "--fp8"
            ],
            "extra_arguments": [
                {
                    "max_input_tokens": "128",
                    "max_new_tokens": "128",
                    "batch_size": "256"
                }
            ]
        }
    ]
}
